\relax 
\ifx\hyper@anchor\@undefined
\global \let \oldcontentsline\contentsline
\gdef \contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global \let \oldnewlabel\newlabel
\gdef \newlabel#1#2{\newlabelxx{#1}#2}
\gdef \newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\let \contentsline\oldcontentsline
\let \newlabel\oldnewlabel}
\else
\global \let \hyper@last\relax 
\fi

\citation{Hinton:2012}
\citation{Wager:2013}
\citation{Clarkson:2012,Nelson:2012}
\gdef\hy@title{The Graduated Dropout: Computation-Risk Tradeoffs Using Data Sparsification}
\thanksnewlabel{e1@email}{{dinah@uchicago.edu}{1}}
\thanksnewlabel{e2@email}{{lafferty@galton.uchicago.edu}{1}}
\thanksnewlabel{e3@email}{{mcallester@ttic.edu}{1}}
\gdef\hy@author{Dinah Shender, John Lafferty and David McAllester}
\gdef\hy@subject{}
\gdef\hy@keywords{}
\gdef\author@num{3}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{Boyd:2011}
\@writefile{toc}{\contentsline {section}{\numberline {2}Computation-Risk Tradeoffs for Dropout and Subsampling}{2}{section.2}}
\newlabel{sec:subsampling}{{2}{2}{Computation-Risk Tradeoffs for Dropout and Subsampling\relax }{section.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Comparision of subsampling to the dropout method for making computation-risk tradeoffs. Two simulations are run, for $p=10$ and $p=20$ variables. Subsampling with different subsample sizes $m$ is compared to the dropout with different dropout rates $\theta $. The computational costs are measured by the theoretical bounds $O(mp^2 + p^3)$ for subsampling and $O(n\delimiter "026B30D \theta \delimiter "026B30D _1 + p^3)$ for the dropout, solved with subspace embedding. As $p$ increases, so does the relative advantage of the dropout.}}{4}{figure.1}}
\newlabel{fig:doss}{{1}{4}{Comparision of subsampling to the dropout method for making computation-risk tradeoffs. Two simulations are run, for $p=10$ and $p=20$ variables. Subsampling with different subsample sizes $m$ is compared to the dropout with different dropout rates $\theta $. The computational costs are measured by the theoretical bounds $O(mp^2 + p^3)$ for subsampling and $O(n\|\theta \|_1 + p^3)$ for the dropout, solved with subspace embedding. As $p$ increases, so does the relative advantage of the dropout}{figure.1}{}}
\bibstyle{imsart-nameyear}
\bibdata{graddrop}
\bibcite{Boyd:2011}{{1}{2011}{{Boyd {\it  et~al.}}}{{}}}
\bibcite{Clarkson:2012}{{2}{}{{Clarkson and Woodruff}}{{}}}
\bibcite{Hinton:2012}{{3}{}{{Hinton {\it  et~al.}}}{{}}}
\bibcite{Nelson:2012}{{4}{}{{Nelson and Nguyen}}{{}}}
\bibcite{Wager:2013}{{5}{}{{Wager, Wang and Liang}}{{}}}
\@writefile{toc}{\contentsline {section}{Acknowledgements}{5}{section*.1}}
\@writefile{toc}{\contentsline {section}{References}{5}{section*.3}}
