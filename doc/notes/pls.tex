\documentclass[12pt]{imsart}
\setattribute{infoline}{text}{file: \jobname.tex\ date: \today}
\def\abstractname{Summary}

\RequirePackage[OT1]{fontenc}
\RequirePackage{amsthm,amsmath,amsfonts,natbib}
%\RequirePackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}
\RequirePackage{hypernat}
\usepackage{times}
%\usepackage[lite,subscriptcorrection,slantedGreek,nofontinfo]{mtpro2}
\bibliographystyle{abbrvnat}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{fullpage}
\usepackage{graphicx,url}

\bibpunct{(}{)}{;}{a}{,}{,}

% settings
%\pubyear{2006}
%\volume{0}
%\issue{0}
%\firstpage{1}
%\lastpage{8}

\startlocaldefs
\numberwithin{equation}{section}
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheoremstyle{remark}{\topsep}{\topsep}%
     {\normalfont}% Body font
     {}           % Indent amount (empty = no indent, \parindent = para indent)
     {\bfseries}  % Thm head font
     {.}          % Punctuation after thm head
     {.5em}       % Space after thm head (\newline = linebreak)
     {\thmname{#1}\thmnumber{ #2}\thmnote{ #3}}% Thm head spec
\theoremstyle{remark}
\newtheorem{remark}{Remark}[section]
\newtheorem{example}{Example}[section]
\newtheorem{assumption}{Assumption}[section]
\newtheorem{definition}{Definition}[section]
\endlocaldefs

\renewcommand{\baselinestretch}{1.1}
\setcounter{tocdepth}{2}
%\parskip12pt
\parindent15pt
\footskip30pt
%\large\normalsize
\def\comma{\unskip,~}
\def\truep{p^*}
\def\div{\|\,}
\long\def\comment#1{}
\def\reals{{\mathbb R}}
\def\P{{\mathbb P}}
\def\E{{\mathbb E}}
\def\supp{\mathop{\text{supp}\kern.2ex}}
\def\argmin{\mathop{\text{\rm arg\,min}}}
\def\arginf{\mathop{\text{\rm arg\,inf}}}
\def\argmax{\mathop{\text{\rm arg\,max}}}
\let\hat\widehat
\let\tilde\widetilde
\def\csd{${}^*$}
\def\mld{${}^\dag$}
\def\dos{${}^\ddag$}
\def\W{\widetilde Y}
\def\Z{\widetilde X}
\let\hat\widehat
\let\tilde\widetilde
\def\ds{\displaystyle}
\def\bs{\backslash}
\def\1{{(1)}}
\def\2{{(2)}}
\def\pn{{(n)}}
\def\ip{{(i)}}
\def\Xbar{\overline{X}}
\def\except{\backslash}
\def\npn{\mathop{\textit{NPN\,}}}
\def\npnsymbol{
  \pspicture(-.2,-.2)(.2,.2)
  \def\cw{.20}
  \cnode[fillstyle=none,linewidth=.5pt](0,0){8pt}{L1}
  \rput(0,0){\tiny\PHhide}
  \psline[linewidth=.5pt,linecolor=black]{-}(-\cw,-\cw)(\cw,\cw)
  \endpspicture
}
\def\i{{(i)}}
\def\cE{{\mathcal{C}}}
\def\cM{{\mathcal{M}}}
\def\cF{{\mathcal{F}}}
\def\cP{{\mathcal{P}}}
\def\cG{{\mathcal{G}}}
\def\M{{\mathcal{M}}}
\def\tr{\mathop{\text{tr}}}
\long\def\comment#1{}
\def\N{\textit{N}\kern.3ex}
\def\t{{\scriptstyle \top}}
\def\fs{\footnotesize}
\let\hat\widehat
\let\epsilon\varepsilon
\let\phi\varphi
\def\ccc{CCC}
\def\N{\mbox{\it N}\,}
\def\NPN{\mathop{\mbox{\it NPN}}\,}
\def\F{\mathcal{F}}
\def\T{\mathcal{T}}
\def\H{\mathcal{H}}
\def\S{S}
\def\X{\mathcal{F}}
\def\Cov{\mathop{\mathbb{C}\textrm{ov}}\kern.1ex}
\def\Var{\mathop{\mathbb{V}\textrm{ar}}\kern.1ex}
\def\had{\!\circ\!}
\def\PNL{X^v}
\def\Cost{C^v}
\def\Value{V^v}
\def\Fill{\iS}
\def\Impacted{\tilde S}
\def\uS{\tilde S}
\def\iS{S^v}
\def\Volume{\tilde N}
\def\uVolume{N}
\def\diag{\text{diag}}
\def\reals{{\mathbb R}}
\def\P{{\mathbb P}}
\def\E{{\mathbb E}}
\def\supp{\mathop{\text{supp}\kern.2ex}}
\def\argmin{\mathop{\text{arg\,min}\kern.2ex}}
\def\argmax{\mathop{\text{arg\,max}\kern.2ex}}
\let\hat\widehat
\let\tilde\widetilde
\def\csd{${}^*$}
\def\mld{${}^\dag$}
\def\dos{${}^\ddag$}
\def\W{\widetilde Y}
\def\Z{\widetilde X}
\def\given{\,|\,}
\def\C{\mathcal{C}}
\def\D{\mathcal{D}}
\def\M{\mathcal{M}}
%\def\N{\mathcal{N}}
\def\N{S}
\def\X{\mathcal{X}}
\def\tr{\mathop{\text{tr}}}
\def\ntr{\mathop{\text{tr}_n}}
\def\ptr{\mathop{\text{tr}_p}}
\def\s{\backslash}
\def\p{\partial}
\def\MS#1{\tilde{#1}}
%\def\MS#1{#1[\uS]}
\def\ones{\text{\bf 1}}
\def\ip#1#2{\langle #1, #2\rangle}
\def\sparsify{\mathop{\mbox{sparsify}}}
\def\Xs{\tilde X}
\parskip12pt
\parindent0pt

\begin{document}

\begin{frontmatter}
%\centerline{\large\bf Log-SOS-Concave Density Estimation and Graphical
%  Modeling}
\end{frontmatter}

\par
%\section{Introduction}
%\label{intro}

%\section{Problem}
\stepcounter{section}

I was looking for a way to incorporate the response in the
sparsification algorithm---just sparsifying the design
is equivalent to a ridge penalty, and doesn't use the response.


The standard partial least squares algorithm is equivalent to
conjugate gradient (see Hastie, Tibshirani \& Friedman):

\begin{center}
\begin{minipage}{.5\columnwidth}
\begin{algorithmic}[1]
\Procedure{PLS}{$X,Y$}
   \State $\hat Y \gets \text{mean}(Y)$
   \For{$m=1,2,\ldots, p$\;}
      \State $\alpha = X^T Y$
      \State $Z = X\alpha$
      \State $Z\gets Z / \|Z\|$
      \State $\hat Y \gets \hat Y + (Z^T Y) Z$
      \State $X \gets X - Z Z^T X$
   \EndFor
   \State \textbf{return} $\hat Y$
\EndProcedure
\end{algorithmic}
\end{minipage}
\end{center}

One idea is to run PLS algorithm while sparsifying $X$
to reduce computation (and regularize):
\begin{center}
\begin{minipage}{.5\columnwidth}
\begin{algorithmic}[1]
\Procedure{PLS}{$X,Y$}
   \State $\hat Y \gets \text{mean}(Y)$
   \State $\Xs = \sparsify(X)$
   \For{$m=1,2,\ldots, T$\;}
      \State $\alpha = \Xs^T Y$
      \State $Z = \Xs \alpha$
      \State $Z\gets Z / \|Z\|$
      \State $\hat Y \gets \hat Y + (Z^T Y) Z$
      \State $\Xs \gets \sparsify(\Xs - Z Z^T \Xs)$
   \EndFor
   \State \textbf{return} $\hat Y$
\EndProcedure
\end{algorithmic}
\end{minipage}
\end{center}
The level of sparsification could change
as the iterations proceed.  

In partial least squares, the estimated coefficients
can be recovered since each operation is linear; the same
holds here.

I implemented this yesterday (March 27) and tried it out on some synthetic data.
The results were not very encouraging--even a small level
of sparsification degraded the performance significantly.

The scenario that motivated AdaGrad intuitively is compatible
with sparsification---if there are frequent but uninformative
features, they should be able to be sparsified with little
loss in predictive accuracy.  On the other hand, rare,
informative features would need to be preserved.  AdaGrad
doesn't explicitly sparsify.

There should be ways of achieving this that are different
than discussed in the Wager, Wang and Liang paper.


\end{document}
