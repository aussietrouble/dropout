\documentclass[12pt]{imsart}
\setattribute{infoline}{text}{file: \jobname.tex\ date: \today}
\def\abstractname{Summary}

\RequirePackage[OT1]{fontenc}
\RequirePackage{amsthm,amsmath,amsfonts,natbib}
%\RequirePackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}
\RequirePackage{hypernat}
\usepackage{times}
%\usepackage[lite,subscriptcorrection,slantedGreek,nofontinfo]{mtpro2}
\bibliographystyle{abbrvnat}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{fullpage}
\usepackage{graphicx,url}
\usepackage{accents}

\bibpunct{(}{)}{;}{a}{,}{,}

% settings
%\pubyear{2006}
%\volume{0}
%\issue{0}
%\firstpage{1}
%\lastpage{8}

\startlocaldefs
\numberwithin{equation}{section}
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheoremstyle{remark}{\topsep}{\topsep}%
     {\normalfont}% Body font
     {}           % Indent amount (empty = no indent, \parindent = para indent)
     {\bfseries}  % Thm head font
     {.}          % Punctuation after thm head
     {.5em}       % Space after thm head (\newline = linebreak)
     {\thmname{#1}\thmnumber{ #2}\thmnote{ #3}}% Thm head spec
\theoremstyle{remark}
\newtheorem{remark}{Remark}[section]
\newtheorem{example}{Example}[section]
\newtheorem{assumption}{Assumption}[section]
\newtheorem{definition}{Definition}[section]
\endlocaldefs

\renewcommand{\baselinestretch}{1.1}
\setcounter{tocdepth}{2}
%\parskip12pt
\parindent15pt
\footskip30pt
%\large\normalsize
\def\mbf#1{\mbox{\boldmath$#1$}}
\def\comma{\unskip,~}
\def\truep{p^*}
\def\div{\|\,}
\long\def\comment#1{}
\def\reals{{\mathbb R}}
\def\P{{\mathbb P}}
\def\E{{\mathbb E}}
\def\supp{\mathop{\text{supp}\kern.2ex}}
\def\argmin{\mathop{\text{\rm arg\,min}}}
\def\arginf{\mathop{\text{\rm arg\,inf}}}
\def\argmax{\mathop{\text{\rm arg\,max}}}
\let\hat\widehat
\let\tilde\widetilde
\def\csd{${}^*$}
\def\mld{${}^\dag$}
\def\dos{${}^\ddag$}
\def\W{\widetilde Y}
\def\Z{\widetilde X}
\let\hat\widehat
\let\tilde\widetilde
\def\ds{\displaystyle}
\def\bs{\backslash}
\def\1{{(1)}}
\def\2{{(2)}}
\def\pn{{(n)}}
\def\ip{{(i)}}
\def\except{\backslash}
\def\npn{\mathop{\textit{NPN\,}}}
\def\npnsymbol{
  \pspicture(-.2,-.2)(.2,.2)
  \def\cw{.20}
  \cnode[fillstyle=none,linewidth=.5pt](0,0){8pt}{L1}
  \rput(0,0){\tiny\PHhide}
  \psline[linewidth=.5pt,linecolor=black]{-}(-\cw,-\cw)(\cw,\cw)
  \endpspicture
}
\def\i{{(i)}}
\def\cE{{\mathcal{C}}}
\def\cM{{\mathcal{M}}}
\def\cF{{\mathcal{F}}}
\def\cP{{\mathcal{P}}}
\def\cG{{\mathcal{G}}}
\def\M{{\mathcal{M}}}
\def\tr{\mathop{\text{tr}}}
\long\def\comment#1{}
\def\N{\textit{N}\kern.3ex}
\def\t{{\scriptstyle \top}}
\def\fs{\footnotesize}
\let\hat\widehat
\let\epsilon\varepsilon
\let\phi\varphi
\def\ccc{CCC}
\def\N{\mbox{\it N}\,}
\def\NPN{\mathop{\mbox{\it NPN}}\,}
\def\F{\mathcal{F}}
\def\T{\mathcal{T}}
\def\H{\mathcal{H}}
\def\S{S}
\def\Cov{\mathop{\mathbb{C}\textrm{ov}}\kern.1ex}
\def\Var{\mathop{\mathbb{V}\textrm{ar}}\kern.1ex}
\def\had{\!\circ\!}
\def\PNL{X^v}
\def\Cost{C^v}
\def\Value{V^v}
\def\Fill{\iS}
\def\Impacted{\tilde S}
\def\uS{\tilde S}
\def\iS{S^v}
\def\Volume{\tilde N}
\def\uVolume{N}
\def\diag{\text{diag}}
\def\reals{{\mathbb R}}
\def\P{{\mathbb P}}
\def\E{{\mathbb E}}
\def\supp{\mathop{\text{supp}\kern.2ex}}
\def\argmin{\mathop{\text{arg\,min}\kern.2ex}}
\def\argmax{\mathop{\text{arg\,max}\kern.2ex}}
\let\hat\widehat
\let\tilde\widetilde
\def\csd{${}^*$}
\def\mld{${}^\dag$}
\def\dos{${}^\ddag$}
\def\W{\widetilde Y}
\def\Z{\widetilde X}
\def\given{\,|\,}
\def\C{\mathcal{C}}
\def\D{\mathcal{D}}
\def\M{\mathcal{M}}
%\def\N{\mathcal{N}}
\def\N{S}
\def\tr{\mathop{\text{tr}}}
\def\ntr{\mathop{\text{tr}_n}}
\def\ptr{\mathop{\text{tr}_p}}
\def\s{\backslash}
\def\p{\partial}
\def\MS#1{\tilde{#1}}
%\def\MS#1{#1[\uS]}
\def\ones{\text{\bf 1}}
\def\ip#1#2{\langle #1, #2\rangle}
\def\sparsify{\mathop{\mbox{sparsify}}}
\def\betas#1{\widehat\beta^{(#1)}}
\def\thetas#1{\theta^{(#1)}}
%\def\Rs#1{\accentset{\circ}{R}^{(#1)}}
\def\Rs#1{{R}^{(#1)}}
\def\Zs#1{\mbf{Z}^{(#1)}}
\def\X{\mbf{X}}
\def\A{\mbf{A}}
\def\S{\mbf{S}}
\def\Xs#1{\accentset{\;\circ}{\mbf{X}}^{(#1)}}
\def\hadamard{\mathop{\raise.5ex\hbox{$\scriptstyle\bullet$}}}
\def\half{{\textstyle\frac{1}{2}}}
\parskip12pt
\parindent0pt

\begin{document}

\begin{frontmatter}
{\bf The Graduated Dropout}
\end{frontmatter}

\par
%\section{Introduction}
%\label{intro}

%\section{Problem}
\stepcounter{section}

We'd like to develop an analyzable procedure that makes a fine-grained computation-risk
tradeoff.   The setting is regression of an $n$-dimensional response $Y$ onto an $n\times p$ design matrix $\X$.
The regime of interest is where $n \gg p$, and also where the
underlying linear model is sparse.

Consider the following \textit{graduated dropout} procedure, where we iteratively
sparsify and fit on the residuals:

\begin{center}
\begin{minipage}{.5\columnwidth}
\begin{algorithmic}[1]
\Procedure{GraduatedDropout}{$\X,Y$}
   \State $\hat Y \gets 0$
   \State $\hat\beta \gets 0$
   \For{$t=1,2,\ldots, T$\;}
      \State $\Rs{t} = Y - \hat Y$
      \State $\Zs{t} \sim \text{Bernoulli}(n, \thetas{t})$
      \State $\Xs{t} = \X \hadamard \Zs{t}$
      \State $\betas{t} = \argmin \|\Rs{t} - \Xs{t} \beta\|^2$
      \State $\hat Y \gets \hat Y + \Xs{t} \betas{t}$
      \State $\hat \beta \gets \hat\beta + \thetas{t} \hadamard \betas{t}$
   \EndFor
   \State \textbf{return} $\hat\beta$
\EndProcedure
\end{algorithmic}
\end{minipage}
\end{center}

In the above pseudocode, $A\hadamard B$ denotes the elementwise
(Hadamard) product of matrices or vectors $A$ and $B$, and $\Zs{t} \sim \text{Bernoulli}(n, \thetas{t})$
is an $n\times p$ matrix of $\{0,1\}$ values, with column $Z_j^{(t)}$ sampled
independently from a $\text{Bernoulli}(\thetas{t}_j)$ distribution.
Thus, $\Xs{t}$ is a sparsified version of the design matrix.
The expected prediction on a new test point $x$ is
\begin{align}
\hat y(x) & = \E \sum_{t=1}^T \bigl(x\hadamard Z^{(t)}\bigr)^T \betas{t} \\
& = \sum_{t=1}^T \bigl(x\hadamard \thetas{t}\bigl)^T \betas{t} \\
& =  x^T \Bigl(\sum_{t=1}^T \thetas{t}\hadamard \betas{t}\Bigr) \\
& = x^T \hat \beta.
\end{align}

Using the
Clarkson-Woodruff line of algorithms for regression with sparse designs, each stage can be carried out
using $O\left(\textit{nnz}(\Xs{t})+p^3\right)$ computation,
where $\textit{nnz}(\mbf{A})$ is the number of nonzeros in the matrix $\mbf{A}$.  Thus,
the total expected computation time scales as 
$$O\left(n\sum_{t=1}^T \|\thetas{t}\|_1 + Tp^3\right).$$

The sparsification can depend on the previous fit--this
is encoded in the parameters $\thetas{t}$.
Interesting variants of this procedure will sparsify in a dynamic
manner.  For example, we can sparsify more ($\thetas{t}_j$ small)
for variables that have small coefficients, or small gradient (change
in the coefficients).  
In the simplest case, we take $\thetas{t}=\theta$ to be constant, and
fix the number of iterations $T$.


The following plot shows the tradeoff in a simulation for a regression problem with
sample size $n=10{,}000$, dimension $p=10$, and number of relevant
variables $s=3$.  We take $T=5$.  

\begin{center}
  \includegraphics[width=.6\textwidth]{risk-theta}
\end{center}

The risk is computed as
\begin{equation}
\text{Risk}(\hat \beta) = \frac{\|\beta^* - \hat\beta\|_{\Sigma}^2}{\|\beta^*\|_{\Sigma}^2}
\end{equation}
where $\Sigma$ is the population covariance and $\beta^*$ is the true
linear model. Adaptively adjusting $\thetas{t}$ should give a more
favorable tradeoff.

Po-Ling Loh and Martin Wainwright's recent work on corrupted data may
give some of the tools needed for analyzing this procedure.


\section*{Wednesday, April 16}

Our intuition is that we should be able to sparsify more over
columns of $\X$ corresponding to irrelevant variables.   We need a simple, concrete
procedure that will lend itself to analysis, in terms
of both computation and statistical risk.

Recall the ADMM algorithm for solving the lasso.  For the lasso
optimization
\begin{align}
\min_\beta \half \|Y - \X\beta\|_2^2 + \lambda\|\beta\|_1
\end{align}
the corresponding ADMM formulation is
\begin{align}
\min_{\beta,z} & \;\; \half \|Y - \X\beta\|_2^2 + \lambda\|z\|_1 \\
\text{such that} & \;\; \beta-z=0.
\end{align}
The ADMM optimization is to iterate the following steps:
\begin{eqnarray}
\beta &\gets& (\X^T \X + \rho I)^{-1} (\X^T Y + \rho z - w) \\
z &\gets& \mathcal{S}_{\lambda/\rho} (\beta + w/\rho)\\
w &\gets& w + \rho (\beta-z)
\end{eqnarray}
where $\mathcal{S}_{\lambda/\rho}$ denotes soft thresholding, 
and $\rho$ is a design parameter in the algorithm.

This suggests a sparsified version:
\begin{eqnarray}
\beta &\gets& (\Xs{t}{}^T \Xs{t} + \rho I)^{-1} (\Xs{t}{}^T Y + \rho z
- w)  \label{eq:ridge}\\
z &\gets& \mathcal{S}_{\lambda/\rho} (\beta + w/\rho)\\
w &\gets& w + \rho (\beta-z)
\end{eqnarray}
where the sparsification $\Xs{t}$ depends on the 
current solution $z$ in iteration $t$.  The linear system \eqref{eq:ridge}
could presumably be solved efficiently using Clarkson-Woodruff.  The
challenge would be to analyze the error that results from sparsification.

I implemented this today, and played around with some simulations.  It
appears promising.

\section*{Wednesday, April 23}

The sparsified ADMM iteration requires the solution to the linear system
\begin{eqnarray}
\beta &\gets& (\Xs{t}{}^T \Xs{t} + \rho I)^{-1} (\Xs{t}{}^T Y + \rho z.
- w)  \label{eq:ridge}
\end{eqnarray}
Adopting the notation of Clarkson-Woodruff, we write this as
\begin{eqnarray}
(\A^T \A + \rho I)\beta = \A^T Y + v
\end{eqnarray}
where $\rho z - w \equiv v$ is a $d$-vector and $\A$ 
is a sparse $n\times d$ matrix. We then compute a sparse
$k\times d$ projection matrix $\S$ so that $\S\A$ can
be computed in time $O(\textit{nnz}(\mbf{A}))$.
We then solve the system
\begin{eqnarray}
(\A^T\S^T \S\A + \rho I)\beta = \A^T Y + v
\end{eqnarray}
in time $O(\textit{nnz}(\mbf{A}) + d^3)$ to compute our approximation.

\end{document}
